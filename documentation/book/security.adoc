== Security

The Cluster Operator provides data encryption and authentication for both Apche Kafka and Apache Zookeeper by means of the SSL/TLS protocol.
This makes it possible to encrypt data transferred between Kafka brokers (interbroker communication), Zookeeper nodes, and clients and Kafka brokers.

The Cluster Operator sets up the SSL/TLS certificates to provide this encryption and authentication.

=== Certificates

Each Kafka broker needs its own private and public keys in order to support encryption.
The public key has to be signed by a certificate authority (CA) in order to have a related X.509 certificate for providing server authentication and encrypting the communication channel with the client (which could be another broker as well).
More precisely, each broker has two different certificates (and related private keys), one for interbroker communication  and one for communication with clients.
These broker certificates are signed using different CAs: The "internal-ca" signs the certificates used for interbroker communication, and the "clients-ca" signs the certificates used for client communication.
The CAs themselves use self-signed certificates.
The "internal-ca" is used across multiple clusters, that is, there is a single "internal-ca" for each instance of a cluster operator. 
The "clients-ca" is specific to a particular Kafka cluster.

All the generated certificates are saved as Secrets in the {ProductPlatformName} cluster, named as follows:

* `internal-ca`: contains the private and public keys, so the self-signed certificate, used for signing broker certificates used for interbroker communication. It is common to all the Kafka clusters deployed by the Cluster Operator.
* `<cluster-name>-kafka-clients-ca`: contains the private and public keys, so the self-signed certificate, used for signing broker certificates used for communicating with clients. It is specific for each deployed Kafka cluster as specified in the <cluster-name> prefix.
* `<cluster-name>-kafka-cert`: contains only the public keys, so the self-signed certificate, used for signing broker certificates used for communicating with clients. This Secret should be used by the final Kafka cluster user in order to extract this certificate to put into the the client's truststores for verifying broker identities (server authentication).
* `<cluster-name>-kafka-brokers-internal`: contains all the brokers private and public keys (certificates signed with "internal-ca") used for interbroker communication.
* `<cluster-name>-kafka-brokers-clients`: contains all the brokers private and public keys (certificates signed with specific cluster "clients-ca") used for communicating with clients.

All the keys are 2048 bits in size and are valid for 365 days from initial generation.

NOTE: "certificates rotation" for generating new ones on their expiration will be supported in future releases.

=== Listeners

The data encryption is provided on two different listeners exposed by each Kafka broker.

Interbroker communication is through the `REPLICATION` listener on port 9091, which is encrypted by default.

Encrypted communication with clients is provided through the `CLIENTTLS` listener on port 9093.

NOTE: Un-encrypted communication with clients is still possible through the `CLIENT` listener on port 9092.

=== Clients connection via TLS

If a Kafka client wants to connect to the encrypted listener (CLIENTTLS) on port 9093, it needs to trust the client's CA certificate in order to verify the broker certificate received during the SSL/TLS handshake.
The clients CA certificate can be extracted from the generated `<cluster-name>-kafka-cert` Secret with the following command if the Kafka cluster is running on {OpenShiftName}.

[source,shell]
oc get secret <cluster-name>-kafka-cert -o jsonpath='{.data.clients-ca\.crt}' | base64 -d > clients-ca.crt

ifdef::Kubernetes[]
If the Kafka cluster is running on {KubernetesName}, the same result can be achieved with the following command.

[source,shell]
kubectl get secret <cluster-name>-kafka-cert -o jsonpath='{.data.clients-ca\.crt}' | base64 -d > clients-ca.crt

endif::Kubernetes[]
The native Kafka client, in order to use it, needs the certificate to be imported in a truststore using the `keytool` command line tool.

[source,shell]
keytool -keystore client.truststore.jks -alias CARoot -import -file clients-ca.crt

Finally, in order to configure the Kafka client, following properties should be specified:

* `security.protocol`: SSL is the value for using encryption.
* `ssl.truststore.location`: the truststore location where the certificates were imported.
* `ssl.truststore.password`: password for accessing the truststore. This property can be omitted if it is not needed by the truststore.

The current implementation does not support Subject Alternative Names (SAN) so the hostname verification should be disabled on the client side.
For doing so the `ssl.endpoint.identification.algorithm` property needs to be set as empty.

=== Secure Zookeeper Communication

By deploying a Stunnel sidecar within every Zookeeper pod, the Cluster Operator is able to provide data encryption and authentication between Zookeeper nodes in a cluster.
The Stunnel sidecar proxies all Zookeeper traffic, TLS decrypting data upon entry into a Zookeeper pod and TLS encrypting data upon departure from a Zookeeper pod.
This TLS encrypting Stunnel proxy is instantiated from the `stunnelImage` specified in the Zookeeper Custom Resource Definition.
