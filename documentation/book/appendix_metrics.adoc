[appendix]
[id='metrics-{context}']
== Metrics

This section describes how to monitor {ProductName} Kafka and ZooKeeper clusters using Grafana dashboards.
To run the example dashboards you must configure a Prometheus server and add the appropriate xref:ref-metrics-deployment-configuration-kafka[metrics configuration] to your Kafka cluster resource.

WARNING: The resources referenced in this section serve as a good starting point for setting up monitoring, but they are provided as an example only.
If you require further support on configuration and running Prometheus or Grafana in production then please reach out to their respective communities.

ifdef::InstallationAppendix[]
When adding Prometheus and Grafana servers to an Apache Kafka deployment using `minikube` or `minishift`, the memory available to the virtual machine should be increased (to 4 GB of RAM, for example, instead of the default 2 GB). Information on how to increase the default amount of memory can be found in the following sections xref:installing_kubernetes_cluster[] and xref:installing_openshift_cluster[].
endif::InstallationAppendix[]

=== Kafka Metrics Configuration

{ProductName} uses the link:https://github.com/prometheus/jmx_exporter[Prometheus JMX Exporter^] to export JMX metrics from Kafka and ZooKeeper to a Prometheus HTTP metrics endpoint that is scraped by Prometheus server.
The Grafana dashboard relies on the Kafka and ZooKeeper Prometheus JMX Exporter relabeling rules defined in the example `Kafka` resource configuration in link:https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/kafka/kafka-metrics.yaml[`kafka-metrics.yaml`^].
Copy this configuration to your own `Kafka` resource definition, or run this example, in order to use the provided Grafana dashboards.

==== Deploying the example Kafka cluster

To deploy the example Kafka cluster the following command should be executed:

[source,shell,subs="+quotes,attributes+"]
kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/kafka/kafka-metrics.yaml

=== Prometheus

link:https://prometheus.io/[Prometheus^] is an open-source set of components for systems monitoring and alerting.
{ProductName} uses the link:https://github.com/coreos/prometheus-operator[CoreOS Prometheus Operator^] to run Prometheus on Kubernetes.
This Operator enables you to run a highly available Prometheus server that is suitable for use in production environments.

==== Deploying the Prometheus Operator on Kubernetes

To deploy the Prometheus Operator in your Kafka cluster, apply the `.yaml` files from the https://github.com/coreos/prometheus-operator/tree/master/example/rbac/prometheus-operator[Prometheus CoreOS repository].

To use a different namespace than that specified in the repository files (`myproject`), use the following commands to download and edit the files from the repository:

On Linux, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-deployment.yaml | sed -e 's/namespace: .\*/namespace: _my-namespace_/' > prometheus-operator-deployment.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role-binding.yaml | sed -e 's/namespace: .*/namespace: _my-namespace_/' > prometheus-operator-cluster-role-binding.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-service-account.yaml | sed -e 's/namespace: .*/namespace: _my-namespace_/' > prometheus-operator-service-account.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role.yaml > prometheus-operator-cluster-role.yaml

On MacOS, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-deployment.yaml | sed -e '' 's/namespace: .\*/namespace: _my-namespace_/' > prometheus-operator-deployment.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role-binding.yaml | sed -e '' 's/namespace: .*/namespace: _my-namespace_/' > prometheus-operator-cluster-role-binding.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-service-account.yaml | sed -e '' 's/namespace: .*/namespace: _my-namespace_/' > prometheus-operator-service-account.yaml
curl -s https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/rbac/prometheus-operator/prometheus-operator-cluster-role.yaml > prometheus-operator-cluster-role.yaml

If needed, you can remove the `securityContext` from the Prometheus Operator `Deployment`.
You can manually remove the `spec.template.spec.securityContext` property from the `prometheus-operator-deployment.yaml` file.

Next, apply all the files using the following commands:

[source,shell,subs=+quotes]
kubectl apply -f prometheus-operator-service-account.yaml
kubectl apply -f prometheus-operator-cluster-role.yaml
kubectl apply -f prometheus-operator-cluster-role-binding.yaml
kubectl apply -f prometheus-operator-deployment.yaml

The {ProductName} repository contains link:https://github.com/strimzi/strimzi-kafka-operator/tree/{GithubVersion}/metrics/examples/prometheus/install[configuration files^] for the Prometheus server. When you apply these configuration files, the following resources are created in your Kubernetes cluster and managed by the Prometheus Operator.

* A `ClusterRole` that grants permissions to read the Prometheus health endpoints of the Kubernetes system, including cAdvisor and the kubelet for container metrics. The Prometheus server configuration uses the Kubernetes service discovery feature in order to discover the pods in the cluster from which it gets metrics.  For this feature to work correctly, the service account used for running the Prometheus service pod must have access to the API server so it can retrieve the pod list.
* A `ServiceAccount` for the Prometheus pods to run under.
* A `ClusterRoleBinding` which binds the aforementioned `ClusterRole` to the `ServiceAccount`.
* A `Deployment` to manage the Prometheus Operator pod.
* A `ServiceMonitor` to manage the configuration of the Prometheus pod.
* A `Secret` to manage additional Prometheus settings.
* A `PrometheusRule` to manage alert rules for the Prometheus pod.
* A `Prometheus` to manage the configuration of the Prometheus pod.

==== Deploying Prometheus

The provided link:https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus.yaml[`prometheus.yaml`^] file, together with the Prometheus related resources, creates a `ClusterRoleBinding` in the `myproject` namespace.
It also discovers an Alertmanager instance in the same namespace.
If you are using a different namespace, download the resource file and update it using the following command:

On Linux, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus.yaml | sed -e 's/namespace: .*/namespace: _my-namespace_/' > prometheus.yaml

On MacOS, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus.yaml | sed -e '' 's/namespace: .*/namespace: _my-namespace_/' > prometheus.yaml

To define Prometheus jobs that will scrape the metrics data, you must apply the `ServiceMonitor` resource located in the provided `strimzi-service-monitor.yaml` file.
Download this file using the following command:

[source,shell,subs=+quotes]
curl -O https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/strimzi-service-monitor.yaml

Currently, the Prometheus Operator only supports jobs that include an `endpoints` role for service discovery.
To use another role, edit the `additionalScrapeConfigs` property in the `prometheus.yaml` configuration file.
This takes the name of the `Secret` and the name of the property in a given `Secret` in which additional configuration is stored.
To create this `Secret` resource, use the following command:

[source,shell,subs="+quotes,attributes+"]
curl -O https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/additional-properties/prometheus-additional.yaml
kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml

The provided `prometheus-rules.yaml` file creates a `PrometheusRule` with sample alerting rules. Download and update the resource file as follows:

On Linux, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus-rules.yaml | sed -e 's/namespace: .*/namespace: _my-namespace_/' > prometheus-rules.yaml

On MacOS, use:

[source,shell,subs=+quotes]
curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus-rules.yaml | sed -e '' 's/namespace: .*/namespace: _my-namespace_/' > prometheus-rules.yaml

To deploy these resources, run the following commands:

[source,shell,subs=+quotes]
kubectl apply -f strimzi-service-monitor.yaml
kubectl apply -f prometheus-rules.yaml
kubectl apply -f prometheus.yaml

NOTE: On OpenShift you need suitable privileges to do this.
On {Minishift}, log in with cluster administrator privileges by executing `oc login -u system:admin` before using the `kubectl apply` commands.

Prometheus also provides an alerting system through the link:https://prometheus.io/docs/alerting/alertmanager/[Alertmanager^] component.
To enable alerting, the provided link:https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus-rules.yaml[`prometheus-rules.yaml`^] file describes a `PrometheusRule` resource that defines sample alerting rules for Kafka and Zookeeper metrics.
When an alert condition is evaluated as true on the Prometheus server, it sends the alert data to the Alertmanager which then uses the configured notification methods to notify the user.

For more information about setting up alerting rules, see https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[Alerting Rules] in the Prometheus documentation.

=== Grafana

A Grafana server is necessary to get a visualisation of the Prometheus metrics.  The source for the Grafana docker image used can be found in the `./metrics/examples/grafana/grafana-openshift` directory.

==== Deploying Grafana

To deploy Grafana the following commands should be executed:

[source,shell,subs="+quotes,attributes+"]
kubectl apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/grafana/grafana.yaml

=== Grafana dashboard

As an example, and in order to visualize the exported metrics in Grafana, two sample dashboards are provided https://github.com/strimzi/strimzi-kafka-operator/blob/{GithubVersion}/metrics/examples/grafana/strimzi-kafka.json[`strimzi-kafka.json`] and https://github.com/strimzi/strimzi-kafka-operator/blob/{GithubVersion}/metrics/examples/grafana/strimzi-zookeeper.json[`strimzi-zookeeper.json`].
These dashboards represent a good starting point for key metrics to monitor Kafka and ZooKeeper clusters, but depending on your infrastructure you may need to update or add to them.
Please note that they are not representative of all the metrics available.
No alerting rules are defined.

The Grafana Prometheus data source, and the above dashboards, can be set up in Grafana by following these steps.

NOTE: For accessing the dashboard, use the `port-forward` command for forwarding traffic from the Grafana pod to the host. For example, access the Grafana UI by running `kubectl port-forward grafana-1-fbl7s 3000:3000` and then pointing a browser to `http://localhost:3000`.

. Access to the Grafana UI using `admin/admin` credentials.  On the following view you can choose to skip resetting the admin password, or set it to a password you desire.
+
image::grafana_login.png[Grafana login]

. Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.
+
image::grafana_home.png[Grafana home]

. Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type. In the URL field, the connection string to the Prometheus server (that is, `http://prometheus-operated:9090`) should be specified. After "Add" is clicked, Grafana will test the connection to the data source.
+
image::grafana_prometheus_data_source.png[Add Prometheus data source]

. From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window where the provided https://github.com/strimzi/strimzi-kafka-operator/blob/{GithubVersion}/metrics/examples/grafana/strimzi-kafka.json[`strimzi-kafka.json`] and https://github.com/strimzi/strimzi-kafka-operator/blob/{GithubVersion}/metrics/examples/grafana/strimzi-zookeeper.json[`strimzi-zookeeper.json`] files can be imported or their content pasted.
+
image::grafana_import_dashboard.png[Add Grafana dashboard]

. After importing the dashboards, the Grafana dashboard homepage will now list two dashboards for you to choose from.  After your Prometheus server has been collecting metrics for a {ProductName} cluster for some time you should see a populated dashboard such as the examples list below.

==== Kafka Dashboard

image::grafana_kafka_dashboard.png[Kafka dashboard]

==== ZooKeeper Dashboard

image::grafana_zookeeper_dashboard.png[ZooKeeper dashboard]

==== Metrics References

To learn more about what metrics are available to monitor for Kafka, ZooKeeper, and Kubernetes in general, please review the following resources.

* http://kafka.apache.org/documentation/#monitoring[Apache Kafka Monitoring] - A list of JMX metrics exposed by Apache Kafka.
It includes a description, JMX mbean name, and in some cases a suggestion on what is a normal value returned.
* https://zookeeper.apache.org/doc/current/zookeeperJMX.html[ZooKeeper JMX] - A list of JMX metrics exposed by Apache ZooKeeper.
* https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/[Prometheus - Monitoring Docker Container Metrics using cAdvisor] - cAdvisor (short for container Advisor) analyzes and exposes resource usage (such as CPU, Memory, and Disk) and performance data from running containers within pods on Kubernetes.
cAdvisor is bundled along with the kubelet binary so that it is automatically available within Kubernetes clusters.
This reference describes how to monitor cAdvisor metrics in various ways using Prometheus.
** https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md[cAdvisor Metrics] - A full list of cAdvisor metrics as exposed through Prometheus.

=== Prometheus alerting

In the monitoring space, one of the useful aspects is to be notified when some metrics conditions are verified.
They allow a human operator to get notifications about problems in the monitored system.

Prometheus allows to write so called "alerting rules" which describe such a conditions using https://prometheus.io/docs/prometheus/latest/querying/basics/[PromQL] expressions that are continuously evaluated.
When an expression becomes true, the described condition is met and the Prometheus server fires an alert.

Prometheus itself is not responsible for sending notifications to the users when an alert is fired.
A different component, the Prometheus Alertmanager, is in charge to do so, sending emails, chat messages or using different notification methods.
When an alert condition is verified, the alert is fired and the Prometheus server sends it to the Alertmanager which will send notifications.

=== Prometheus Alertmanager

Other than a server for scraping metrics, Prometheus provides an alerting system through the Alertmanager component.
It is possible to declare alerting rules on the Prometheus server in order to be notified about specific conditions in the metrics.
When an alert condition is evaluated as true, Prometheus sends alert data to the Alertmanager which then sends notifications out.
Notifications can be sent via methods such as email, Slack, PagerDuty and HipChat

The provided Prometheus https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/alert-manager.yaml[`alert-manager.yaml`] file describes the resources required for deploying and configuring the Alertmanager.
The file https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/alertmanager-config/alertmanager.yaml[`alertmanager.yaml`] YAML file describes the hook for sending notifications.

The following resources are defined:

* An `Alertmanager` to manage the Alertmanager pod.
* A `Secret` to manage the configuration of the Alertmanager.
* A `Service` to provide an easy to reference hostname for other services to connect to Alertmanager (such as Prometheus).

The provided sample configuration configures the Alertmanager to send notification to a Slack channel.
Before deploying the Alertmanager it is needed to update the following parameters:

* The `slack_api_url` field with the actual value of the Slack API URL related to the application for the Slack workspace.
* The `channel` field with the actual Slack channel on which sending the notifications.

==== Deploying Alertmanager

Download `alert-manager.yaml` by a command.

[source,shell,subs=+quotes]
curl -O https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/alert-manager.yaml

To configure Alert Manager hook for sending alerts we need to create a `Secret` resource with configuration.
Download the `alertmanager.yaml` file and create a `Secret` from it.

This can be done using these commands:
[source,shell,subs="+quotes,attributes+"]
curl -O https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/alertmanager-config/alertmanager.yaml
kubectl create secret generic alertmanager-alertmanager --from-file=alertmanager.yaml

To deploy the Alertmanager the following commands should be executed:
This can be done using `kubectl apply`:
[source,shell,subs="+quotes,attributes+"]
kubectl apply -f alert-manager.yaml

==== Alerts examples

The provided https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/{GithubVersion}/metrics/examples/prometheus/install/prometheus-rules.yaml[`prometheus-rules.yaml`] file provides the following sample alerting rules on Kafka and Zookeeper metrics.

Kafka alerts are:

* `UnderReplicatedPartitions`: the under replicated partitions metric gives the number of partitions for which the current broker is the leader replica but the follower replicas are not caught up.
This metric provides insights about offline brokers which hosts the follower replicas.
This alert is raised when this value is greater than zero, providing the information of the under replicated partitions for each broker.

* `AbnormalControllerState`: the active controller metric indicate if the current broker is the controller for the cluster.
It can just be 0 or 1. 
During the life of a cluster, only one broker should be the controller and the cluster needs to have always an active controller.
Having two or more brokers saying that they are controllers indicates a problem.
This alert is raised when the sum of all the values for this metric on all broker is not equals to 1.
It means that there is no active controller (the sum is 0) or more than one controller (the sum is greater than 1).

* `UnderMinIsrPartitionCount`: the Kafka broker `min.insync.replicas` allows to specify the minimum number of replicas that have to acknowledge a write operation for successful in order to be in-sync.
The under min ISR partition count metric defines the number of partitions that this broker leads for which in-sync replicas count is less than the min in-sync.
This alert is raised when this value is greater than zero, providing the information of the under min ISR partition count for each broker.

* `OfflineLogDirectoryCount`: the offline log directory count metric indicate the number of log directories which are offline (due to an hardware failure for example) so that the broker cannot store incoming messages anymore.
This alert is raised when this value is greater than zero, providing the information of the number of offline log directories for each broker.

* `KafkaRunningOutOfSpace`: the running out of space metric indicates the remaining amount of disk space that can be used for writing Kafka's data.
This alert is raised when this value is lower than 5GiB. It provides information on the disk that is running out of space for each persistent volume claim.
NOTE: The availability of this metric and alert is dependent on your version of Kubernetes.

Zookeeper alerts are:

* `AvgRequestLatency`: the average request latency metric indicates the amount of time it takes for the server to respond to a client request.
This alert is raised when this value is greater than 10 (ticks), providing the actual value of the average request latency for each server.

* `OutstandingRequests`: the outstanding requests metric indicates the number of queued requests in the server.
This value goes up when the server receives more requests than it can process.
This alert is raised when this value is greater than 10 (ticks), providing the actual number of outstanding requests for each server.

* `ZookeeperRunningOutOfSpace`: the running out of space metric indicates the remaining amount of disk space that can be used for writing data to Zookeeper.
This alert is raised when this value is lower than 5GiB. It provides information on the disk that is running out of space for each persistent volume claim.
Note: The availability of this metric and alert is dependent on your version of Kubernetes.
